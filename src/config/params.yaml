Linear Regression: {}

Lasso:
  alpha: [0.01, 0.1, 1.0, 10.0]
  max_iter: [1000, 5000, 10000]

Ridge:
  alpha: [0.01, 0.1, 1.0, 10.0]
  solver: ["auto", "svd", "cholesky", "lsqr"]

ElasticNet:
  alpha: [0.01, 0.1, 1.0, 10.0]
  l1_ratio: [0.1, 0.3, 0.5, 0.7, 0.9]
  max_iter: [1000, 5000, 10000]

K-Neighbors Regressor:
  n_neighbors: [3, 5, 7, 9]
  weights: ["uniform", "distance"]
  algorithm: ["auto", "ball_tree", "kd_tree", "brute"]

Decision Tree:
  criterion: ["squared_error", "friedman_mse", "absolute_error", "poisson"]
  max_depth: [null, 5, 10, 20]
  min_samples_split: [2, 5, 10]

Random Forest Regressor:
  n_estimators: [50, 100, 200]
  max_depth: [null, 5, 10, 20]
  min_samples_split: [2, 5, 10]
  max_features: ["sqrt", "log2"]

XGBRegressor:
  n_estimators: [50, 100, 200]
  learning_rate: [0.01, 0.05, 0.1]
  max_depth: [3, 5, 7]

CatBoosting Regressor:
  depth: [4, 6, 8, 10]
  learning_rate: [0.01, 0.05, 0.1]
  iterations: [50, 100, 200]

AdaBoost Regressor:
  n_estimators: [50, 100, 200]
  learning_rate: [0.01, 0.05, 0.1, 1.0]

Gradient Boosting Regressor:
  n_estimators: [50, 100, 200]
  learning_rate: [0.01, 0.05, 0.1]
  subsample: [0.6, 0.75, 0.9]
  max_depth: [3, 5, 7]
